{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaup\n",
    "\n",
    "In this notebook, I will try to use functional programming side of python as much as possible.  \n",
    "As this might make code easier to migrate to Spark platform.   \n",
    "The first cell is to read the simple data file with respect to lines.(Just like what Hadoop MapReduce and Spark do!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of simple data set size: 1001\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from itertools import groupby\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "with open('../data/data_simple.xml', 'r') as f:\n",
    "    raw_data = f.readlines()\n",
    "\n",
    "print('Number of simple data set size: {}'.format(len(raw_data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of records now: 998\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(6,\n",
       "  \"The explicit cast to double in the first answer isn't necessary - identifying the constant as 5000.0 (or as 5000d) is sufficient.\"),\n",
       " (12, 'Binary Data in MYSQL'),\n",
       " (13, 'databasemysql'),\n",
       " (14, 'How do I store binary data in mysql?')]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# use regex to extract our data(id--content pair)\n",
    "id_pattern = re.compile('<row Id=\\\"([\\d]*)\\\"')\n",
    "content_pattern = re.compile('Text=\\\"([\\W\\w]*)\\\"')\n",
    "noise_pattern = re.compile('&[#]*[\\w]*;')\n",
    "\n",
    "def job_filter(input_str: str) -> bool:\n",
    "    return id_pattern.search(input_str) and content_pattern.search(input_str)\n",
    "\n",
    "def job_extract(input_str: str) -> tuple:\n",
    "    postid = id_pattern.search(input_str).group(1)\n",
    "    content = content_pattern.search(input_str).group(1)\n",
    "    content = noise_pattern.sub('', content)\n",
    "    return postid, content\n",
    "\n",
    "def job_cleanup_format(input_tuple: tuple) -> tuple:\n",
    "    postid, content = input_tuple\n",
    "    return int(postid), content.strip()\n",
    "\n",
    "# use functional programming approach as much as possible ... \n",
    "records_has_content = filter(job_filter, raw_data)\n",
    "extracted_records = map(job_extract, records_has_content)\n",
    "id_content_list = map(job_cleanup_format, extracted_records)\n",
    "\n",
    "# make id_content_list a list for inspect\n",
    "# we don't need to change it to list in production\n",
    "id_content_list = list(id_content_list)\n",
    "DOCUMENTS_COUNT = len(id_content_list)\n",
    "print('The number of records now: {}'.format(len(id_content_list)))\n",
    "id_content_list[:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3.48 s, sys: 15.3 ms, total: 3.5 s\n",
      "Wall time: 3.51 s\n"
     ]
    }
   ],
   "source": [
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "stemmer = PorterStemmer()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# to lower case, no punctuation, stemmed, filter stop words\n",
    "# this python version is highly depend on nltk, may consider sth else later\n",
    "def job_split_content(input_tuple: tuple) -> list:\n",
    "    postid, input_str = input_tuple\n",
    "    input_str = input_str.lower()\n",
    "    raw_tokens = tokenizer.tokenize(input_str)\n",
    "    stemmed_tokens = [stemmer.stem(token) for token in raw_tokens]\n",
    "    stemmed_tokens = map(stemmer.stem, raw_tokens)\n",
    "    stemmed_tokens_without_stopword = filter(lambda i: i not in stop_words, stemmed_tokens)\n",
    "    return postid, list(stemmed_tokens_without_stopword)\n",
    "\n",
    "# this step seems to use up a lot of time!\n",
    "%time id_tokens = list(map(job_split_content, id_content_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_words = set()\n",
    "for record in id_tokens:\n",
    "    _, token_list = record\n",
    "    corpus_words.update(token_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_words = list(corpus_words)\n",
    "WORD_COUNT = len(corpus_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3.81 s, sys: 31.7 ms, total: 3.85 s\n",
      "Wall time: 3.85 s\n"
     ]
    }
   ],
   "source": [
    "def job_word_to_index(input_tuple):\n",
    "    postid, tokens = input_tuple\n",
    "    content_indexed = [corpus_words.index(token) for token in tokens] # [index1, index2, index1, index1, index3], for example\n",
    "    content_freq = dict()\n",
    "    for key, grouped in groupby(content_indexed):\n",
    "        content_freq[key] = len(list(grouped))\n",
    "    indexs = set(content_freq.keys())\n",
    "    result_list = list()\n",
    "    for i in range(WORD_COUNT):\n",
    "        if i in indexs:\n",
    "            result_list.append(content_freq.get(i))\n",
    "        else:\n",
    "            result_list.append(0)\n",
    "        \n",
    "    return postid, np.array(result_list)\n",
    "\n",
    "%time id_freq_dicts = list(map(job_word_to_index, id_tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now **id\\_freq\\_dicts** is a term frequency (TF) matrix. Next we will try to convert it into a TF-IDF matrix   \n",
    "I would say IDF is the most time consuming part for now. Because it required aggregation among all records.(which is distributed among clusters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.67 s, sys: 47.7 ms, total: 2.71 s\n",
      "Wall time: 2.68 s\n"
     ]
    }
   ],
   "source": [
    "def job_idf_for_word(word):\n",
    "    word_index = corpus_words.index(word)\n",
    "    \n",
    "    # notice this step is actually performed by spark, not this whole function\n",
    "    # the reason i use map instead of filter is that \n",
    "    # filter will return a huge data structure. but map will just return true or false.\n",
    "    documents_contain_word = np.array(list(map(lambda i:i[1][word_index] > 0, id_freq_dicts)))\n",
    "    # add all the true value up. the result is the document frequency (document count)\n",
    "    document_frequency = np.sum(documents_contain_word)\n",
    "    return math.log(DOCUMENTS_COUNT/document_frequency)\n",
    " \n",
    "# get the idf array\n",
    "%time idf_array = np.array([job_idf_for_word(word) for word in corpus_words])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 24 ms, sys: 21.3 ms, total: 45.2 ms\n",
      "Wall time: 46.7 ms\n"
     ]
    }
   ],
   "source": [
    "# multiple each tf with idf, we get tfidf matrix\n",
    "%time id_tfidf = list(map(lambda input_tuple: (input_tuple[0], input_tuple[1] * idf_array), id_freq_dicts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 5.80714099,  5.51945892,  6.90575328,  3.44001737,  2.46310202,\n",
       "        4.60316818,  5.80714099,  2.86270201,  2.54904445,  6.90575328,\n",
       "        4.82631173,  5.51945892])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id_tfidf[0][1][id_tfidf[0][1]!=0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The approximate time of our cleanup process is about :10.5\n"
     ]
    }
   ],
   "source": [
    "print('The approximate time of our cleanup process is about :{}'.format(3.51+3.85+2.68+0.46))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
